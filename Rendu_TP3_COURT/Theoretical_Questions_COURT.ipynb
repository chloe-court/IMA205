{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the fixed design model, we have :\n",
    "\n",
    "$$\\boxed{Y=X\\beta+\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)}$$   \n",
    "\n",
    "Then we have :  \n",
    "\n",
    "* $\\mathbb{E}[\\tilde{\\beta}]=\\mathbb{E}[Cy]=\\mathbb{E}[(H+D)y]=\\mathbb{E}[Hy]+\\mathbb{E}[Dy]=\\mathbb{E}[\\tilde{\\beta}]+\\mathbb{E}[Dy]=(I_d+Dx)\\beta$\n",
    "\n",
    "As $\\tilde{\\beta}$ is unbiased, $Dx=0$\n",
    "\n",
    "* $\\text{Var}(\\tilde{\\beta}) = \\text{Var}(Cy) = C\\text{Var}C^T=\\sigma^2CC^T=\\sigma^2(HH^T+HD^T+DH^T+DD^T)= \\sigma^2((x^Tx)^{-1}+DD^T)\\quad$ as $Dx=0$.\n",
    "\n",
    "Then $\\text{Var}(\\tilde{\\beta}) = \\text{Var}(\\beta^*) + \\sigma^2DD^T$\n",
    "\n",
    "As $\\sigma^2DD^T=\\sigma^2||D||^2_2 \\geq 0$, which proves that :  \n",
    "\n",
    "$$\\boxed{\\text{Var}(\\tilde{\\beta}) > \\text{Var}(\\beta^*)}$$\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ridge regression  \n",
    "\n",
    "The explicit Ridge solution is the following :\n",
    "\n",
    "$$\\beta^*_{ridge}=(x_c^Tx_c+\\lambda I_d)^{-1}x_c^Ty_c$$\n",
    "\n",
    "**1- Bias :**\n",
    "\n",
    "$\\quad\\mathbb{E}[\\beta^*_{ridge}]=\\mathbb{E}[(x_c^Tx_c+\\lambda I_d)^{-1}x_c^Ty_c]=(x_c^Tx_c+\\lambda I_d)^{-1}X^T\\mathbb{E}[y_c] = (x_c^Tx_c+\\lambda I_d)^{-1}x_c^Tx_c\\beta$\n",
    "\n",
    "It is then biased unless $\\lambda=0$. We proved the ridge regression to be biased.\n",
    "\n",
    "**2- SVD Decomposition**  \n",
    "\n",
    "$\\quad\\beta^*_{ridge}=(x_c^Tx_c+\\lambda I_d)^{-1}x_c^Ty_c=((UDV^T)^TUDV^T+\\lambda I_d)^{-1}(UDV^T)^Ty_c = (VD^2V^T+\\lambda I_d)^{-1}VD^TU^Ty_c= V(D^2+\\lambda I_d)V^TVD^TU^Ty_c=\\boxed{V(D^2+\\lambda I_d)^{-1}D^TU^Ty_c =\\beta^*_{ridge}}$  \n",
    "\n",
    "This decomposition can be useful because we dont need to invert a matrix as $D^2+\\lambda I_d$ is diagonal.\n",
    "\n",
    "**3- Variance**\n",
    "\n",
    "$\\quad\\text{Var}(\\beta^*_{ridge})=\\text{Var}((x_c^Tx_c+\\lambda I_d)^{-1}x_c^Ty_c)=(x_c^Tx_c+\\lambda I_d)^{-1}x_c^T\\text{Var}(y_c)((x_c^Tx_c+\\lambda I_d)^{-1}x_c^T)^T=\\sigma^2(x_c^Tx_c+\\lambda I_d)^{-1}x_c^Tx_c(x_c^Tx_c+\\lambda I_d)^{-1}$\n",
    "\n",
    "This means that if $\\lambda>0$, we have : $\\boxed{\\text{Var}(\\beta^*_{ridge})\\leq \\text{Var}(\\beta^*_{OLS})}$\n",
    "\n",
    "**4- Parameter $\\lambda$**\n",
    "\n",
    "$\\quad\\lim_{\\lambda \\to \\infty} \\mathbb{E}[(\\beta^*_{ridge})]=0\\lim_{\\lambda \\to 0} \\mathbb{E}[\\beta^*_{ridge}]=\\mathbb{E}[(x_c^Tx_c)^{-1}x_c^Ty_c]=\\mathbb{E}[(x_c^Tx_c)^{-1}x_c^Tx_c\\theta]=\\theta=\\mathbb{E}[(\\beta^*_{OLS})]$  \n",
    "\n",
    "$\\quad\\lim_{\\lambda \\to \\infty} \\text{Var}((\\beta^*_{ridge}))=0\\lim_{\\lambda \\to 0} \\text{Var}(\\beta^*_{ridge})=\\text{Var}(\\beta^*_{OLS})=\\sigma^2(x_c^Tx_c)^{-1}$  \n",
    "\n",
    "**5- Relation between ridge and OLS**\n",
    "\n",
    "When $x_c^Tx_c=I_d$, \n",
    "\n",
    "Then : $\\beta^*_{ridge}=(x_c^Tx_c+\\lambda I_d)^{-1}x_c^Ty_c=\\frac 1 {\\lambda + 1}x_c^Ty_c$\n",
    "\n",
    "And : $\\beta^*_{OLS}=(x_c^Tx_c)^{-1}x_c^Ty_c=x_c^Ty_c$\n",
    "\n",
    "This proves that :\n",
    "\n",
    "$$\\boxed{\\beta^*_{ridge} = \\frac {\\beta^*_{OLS}} {\\lambda + 1}}$$\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Elastic Net\n",
    "\n",
    "When $x_c^Tx_c=I_d$, $\\quad\\beta^*_{OLS}=(x_c^Tx_c)^{-1}x_c^Ty_c=x_c^Ty_c$.\n",
    "\n",
    "Let $f$ be the objective function such that $f(\\beta)=||y_c-x_c\\beta||^2 + \\lambda_2||\\beta||^2_2 + \\lambda_1 ||\\beta||_1$\n",
    "\n",
    "To find the minimum, as the function is strictly convex, it only takes to vanish the subgradient of f.\n",
    "\n",
    "$\\quad\\frac{\\partial f}{\\partial x}=2x_c^T(y_c-x_c\\beta)+2\\lambda_2\\beta + \\lambda_1 \\begin{cases}\n",
    "    {-1} & \\text{if } \\beta < 0 \\\\\n",
    "    {1} & \\text{if } \\beta > 0 \\\\\n",
    "    [-1,1] & \\text{if } \\beta = 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Then the minimum is when :\n",
    "\n",
    "$2x_c^T(y_c-x_c\\beta)+2\\lambda_2\\beta \\pm \\lambda_1 = 0$  \n",
    "\n",
    "ie $2x_c^T(y_c-x_c\\beta)+2\\lambda_2\\beta \\pm \\lambda_1 = 0$  \n",
    "\n",
    "so $2\\beta^*_{OLS}-\\beta+2\\lambda_2\\beta \\pm \\lambda_1 = 0$\n",
    "\n",
    "This proves that :\n",
    "\n",
    "$$\\boxed{\\beta^*_{\\text{ElNet}} = \\frac{\\beta^*_{\\text{OLS}} \\pm \\frac{\\lambda_1}{2}}{1 + \\lambda_2}}$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
